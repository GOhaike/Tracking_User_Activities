{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gabriel Ohaike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    " As a data scientist at a game development company, I am interested in tracking two events from my latest mobile game. Buy a sword & join guild. Each has metadata characterstic of such events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks:\n",
    "        \n",
    "**In order to do this**, \n",
    "\n",
    "  1. I will instrument my API server to log events to Kafka\n",
    "  \n",
    "  2. Assemble a data pipeline to catch these events using Spark streaming to filter select event types from Kafka, land them into HDFS/parquet to make them available for analysis using Presto\n",
    "  \n",
    "  3. Use Apache Bench to generate test data for my pipeline.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create A docker Compose file**:\n",
    "\n",
    "The first thing to do is create `docker compose file` that contains all the containers needed to successfully execute the events tracking. The container is made up of **zookeeper, kafka, claudera, spark, presto and mids** container. To see the content structure and port structure please refer to `docker-compose.yml`.\n",
    "\n",
    "**Here is an example of one of the containers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presto:\n",
    "    image: midsw205/presto:0.0.1\n",
    "    hostname: presto\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    expose:\n",
    "      - \"8080\"\n",
    "    environment:\n",
    "      HIVE_THRIFTSERVER: cloudera:9083\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spin up cluster:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To spins up the container.The docker-compose up aggregates the output of each container in the docker-compose.yml file and -d starts the containers in the background and leave them running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a topic:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to create a kafka topic. The exec is use to issue a command expecially when the container is running multiple services. Next, `kafka kafka-topics` tells docker-compose to create a kafka topic. `create --topic events` now create a topc called assessment.partitions 1 allows topics to be parallelized by spitting in data into a particular topic across a multiple brokers. We are only interested in one partition here as per project reqirement, hence the number of `partitions is 1 with the replication factor of 1`. This defines the replication implimented at the partition level. Since we are only interested in one kafka topic, we set our replication factor as 1. --if-not-exists tells the command to execute only if topic does not exist, this avoids errors/warnings.--zookeeper zookeeper:32181 Here the option zookeeper is telling our connection zookeeper to connect to port 32181."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a web-based application:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "producer = KafkaProducer(bootstrap_servers='kafka:29092')\n",
    "\n",
    "\n",
    "def log_to_kafka(topic, event):\n",
    "    event.update(request.headers)\n",
    "    producer.send(topic, json.dumps(event).encode())\n",
    "    \n",
    "@app.route(\"/purchase_a_sword\")\n",
    "def purchase_a_sword():\n",
    "    purchase_sword_event = {'event_type': 'purchase_sword'}\n",
    "    log_to_kafka('events', purchase_sword_event)\n",
    "    return \"Sword Purchased!\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web app is called `game_api.py` in the folder. The code above shows some of the implementation processes. The `game_api.py` contains three main event. To process this, the `mobile app` makes an `API` call to the `web-based API server` with any of the following calls\n",
    "\n",
    "   1. `default responses:`\n",
    "        This returns a default response \"This is the default response \"\n",
    "    \n",
    "   2. `purchase_a_sword:`\n",
    "        This api is called when the user want to purchase a sword. It ruturns \"Sword purchased\"\n",
    "    \n",
    "   3. `join_a_guild:`\n",
    "       This is called when a user want to join a guild. It returns \"Joined a Guild\"\n",
    "   \n",
    " In creating our web-based application, we import `Flask` class and create an instance of class called `app = Flask(__name__)`. We also import `KafkaProducer` to read from kafka using bootstrap.servers configuration to connect to `kafka:29092.` Next, we defined a function `log_to_kafka` to log events to kafka, update event header and use the `send` to send event to kafka `producer` and dump event to `json`before we log it kafka. Encoded with `encode()` for UTF8.  The `route()` decorator tells `Flask` what `url` should trigger the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we need to do is getting our set up ready to stream events. To do this, we file\n",
    "\n",
    "   _1. ab.sh_\n",
    "   \n",
    "   _2. guild_sword_stream.py_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ab.sh:\n",
    "\n",
    "The `ab.sh` file uses `mids` container with `apache bench` denoted as `ab` to generate data. In the code example below, we are simply using `ab` to generate `150` purchases events from `user1` using a localhost:5000. The `.sh` script controls how we want our events to run during streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose exec mids ab -n 150 -H \"Host:user1.comcast.com\" http://localhost:5000/purchase_a_sword\n",
    "docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/join_guild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### guild_sword_stream.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `guild_sword_stream.py` is used to define events schema, filter out events of interest and load into a `json` file and send to spark to extract events and write it into `HDFS`. Let's look at it in details to understand how different pieces contribute to the overall streaming process.\n",
    "\n",
    "\n",
    "\n",
    "This extracted events is now fed to streaming mode as request comes in. Here, because our client is only interested in `sword purchases and join guild`, only these two would be filtered. These filtered events is `cast` to string. The `CAST()` function converts a value (of any type) into a specified datatype. In this case, we are converting value from the root to string. Finally,we `write` the stream events to `HDFS` using a processing time of `10 seconds`."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
