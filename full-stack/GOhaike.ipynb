{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gabriel Ohaike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    " As a data scientist at a game development company, I am interested in tracking two events from my latest mobile game. Buy a sword & join guild. Each has metadata characterstic of such events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "        \n",
    "**In order to do this**, \n",
    "\n",
    "  1. I will instrument my API server to log events to Kafka\n",
    "  \n",
    "  2. Assemble a data pipeline to catch these events using Spark streaming to filter select event types from Kafka, land them into HDFS/parquet to make them available for analysis using Presto\n",
    "  \n",
    "  3. Use Apache Bench to generate test data for my pipeline.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Docker Compose file**:\n",
    "\n",
    "The first thing to do is create `docker compose file` that contains all the containers needed to successfully execute the events tracking. The container is made up of **zookeeper, kafka, claudera, spark, presto and mids** container. To see the content structure and port numbers please refer to `docker-compose.yml`.\n",
    "\n",
    "**Here is an example of one of the containers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presto:\n",
    "    image: midsw205/presto:0.0.1\n",
    "    hostname: presto\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    expose:\n",
    "      - \"8080\"\n",
    "    environment:\n",
    "      HIVE_THRIFTSERVER: cloudera:9083\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spin up cluster:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the docker compose file, the next thing to do is spin up the container. The docker-compose up aggregates the output of each container in the docker-compose.yml file and -d starts the containers in the background and leave them running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a topic:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above creates a kafka topic. The `exec` is use to issue a command expecially when the container is running multiple services like is here. Next, `kafka kafka-topics` tells docker-compose to create a kafka topic. `create --topic events` now create a topc called assessment.partitions 1 allows topics to be parallelized by spitting in data into a particular topic across a multiple brokers. We are only interested in one partition here as per project reqirement, hence the number of `partitions is 1 with the replication factor of 1`. This defines the replication implimented at the partition level. Since we are only interested in one kafka topic, we set our replication factor as 1. --if-not-exists tells the command to execute only if topic does not exist, this avoids errors/warnings.--zookeeper zookeeper:32181 Here the option zookeeper is telling our connection zookeeper to connect to port 32181."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a web-based application:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web-base applications is the powerhouse for our `game_api`. The `app` which is specifically built for selected events based on anticipated user request is implemented using flask and kafka producers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "producer = KafkaProducer(bootstrap_servers='kafka:29092')\n",
    "\n",
    "\n",
    "def log_to_kafka(topic, event):\n",
    "    event.update(request.headers)\n",
    "    producer.send(topic, json.dumps(event).encode())\n",
    "    \n",
    "@app.route(\"/purchase_a_sword\")\n",
    "def purchase_a_sword():\n",
    "    purchase_sword_event = {'event_type': 'purchase_sword'}\n",
    "    log_to_kafka('events', purchase_sword_event)\n",
    "    return \"Sword Purchased!\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web app is called `game_api.py` in the folder. The code above shows some of the implementation processes. The `game_api.py` contains three main event. To process this, the `mobile app` makes an `API` call to the `web-based API server` with any of the following calls\n",
    "\n",
    "   1. `default responses:`\n",
    "        This returns a default response \"This is the default response \"\n",
    "    \n",
    "   2. `purchase_a_sword:`\n",
    "        This api is called when the user want to purchase a sword. It ruturns \"Sword purchased\"\n",
    "    \n",
    "   3. `join_a_guild:`\n",
    "       This is called when a user want to join a guild. It returns \"Joined a Guild\"\n",
    "   \n",
    " In creating our web-based application, we import `Flask` class and create an instance of class called `app = Flask(__name__)`. We also import `KafkaProducer` to read from kafka using bootstrap.servers configuration to connect to `kafka:29092.` Next, we defined a function `log_to_kafka` to log events to kafka, update event header and use the `send` to send event to kafka `producer` and dump event to `json`before we log it kafka. Encoded with `encode()` for UTF8.  The `route()` decorator tells `Flask` what `url` should trigger the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we need to do is getting our set up ready to stream events. \n",
    "\n",
    "To do this, we implement the following files before\n",
    "\n",
    "   **1. ab.sh**\n",
    "   \n",
    "   **2.event_stream.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ab.sh:\n",
    "\n",
    "The `ab.sh` file uses `mids` container with `apache bench` denoted as `ab` to generate data. In the code example below, we are simply using `ab` to generate `150` purchases events from `user1` using a localhost:5000. The `.sh` script controls how we want our events to run during streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose exec mids ab -n 150 -H \"Host:user1.comcast.com\" http://localhost:5000/purchase_a_sword\n",
    "docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/join_guild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a more closer look at the this later in this report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### event_stream.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `event_stream.py` is used to define events schema, filter out events of interest and load into a `json` file, send to spark to extract events and write it into `HDFS`. Let's look at it in details to understand how different pieces contribute to the overall streaming process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define events schema:\n",
    "\n",
    "The code below defines our `schema`. The `StructType` objects is used to define the schema of `Accept,Host, User_Agent, event_type` and flag the nullable for each column in the dataframe to be true. This leads to smooth transition to our `json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sword_purchase_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extracted events is now fed to streaming mode as request comes. Here, because we are only interested in `sword purchases and join guild`, only these two would be filtered. These filtered events is `cast` to string. The `CAST()` function converts a value (of any type) into a specified datatype. In this case, we are converting value from the root to string. Finally, we `write` the stream events to `HDFS` using a processing time of `10 seconds`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@udf` takes boolean values that returns `True or False` . The `join_guild` function takes events in json file format and extracts `join guild`. The return `True` returns only `join guild` and filter out any other events. The same is applied to `sword purchases`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf('boolean')\n",
    "def join_guild(event_as_json):\n",
    "    \"\"\"udf for filtering events\n",
    "    \"\"\"\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] in ('purchase_sword','join_guild'):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event streaming is done by using a `SparkSession` to get or create events. To get the job to spark, we need the spark context using spark session that handles the background task. These events are then read by `kafka` using a `readStream` to read stream events as they are being fed from `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ExtractEventsJob\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    raw_events = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The streaming events from spark is fed to filter `sword_purchases and join_guild` cast them into string and select the filtered events with time stamp from the `json` file as seen in the code below then infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    join_guild = raw_events \\\n",
    "        .filter(is_guild(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          join_guild_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sink to hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These events are then written to hdfs using parquet format. The `writestream` writes the stream events after 10 seconds and stores in the `hdfs`. Once all events are written to `hdfs` it terminates using `sink_guild.awaitTermination()`. It is advisable to set the `processing time` to reasonable time in order not to store events in too many small chunks of codes to the `hdfs` or keep so much memory in spark. Checkpoints is saved as `checkpoints_join_guild`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sink = join_guild \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_join_guild\") \\\n",
    "        .option(\"path\", \"/tmp/join_guild\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    sink_guild.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haven looked into different files usage, let's put these piece together by taking a walk through of the execution process. Some of the codes needed to  see to run these events and store in the `hdfs` also query using the presto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After spinning up the `docker-compose.yml` file. Run the flask using `mids` container using flask envirnment variable `FLASK_APP` to run `game_api.py` using the host option `0.0.0.0.` that allows us to connect to `flask` from external sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose exec mids env FLASK_APP=/w205/project-3-GOhaike/full-stack/game_api.py flask run --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will continue running while we set up kafkacat from the beginning to continously running mode. This is will enable streaming as the events comes in while `kafka` is listening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While kafka is listening, we run Apache bench `ab.sh` to generate our streaming events and send in to `kafkacat`. The `while loop` ensures events are stream every 10 mins using `ab` in mids container and send to `kafka`. In our case here, we have `three users` for sword purchases and join guild. Running the file will generate test data to our pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while true\n",
    "\tdo\n",
    "\tdocker-compose exec mids ab -n 150 -H \"Host:user1.comcast.com\" http://localhost:5000/purchase_a_sword\n",
    "\tdocker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/join_guild\n",
    "\tdocker-compose exec mids ab -n 100 -H \"Host: user2.comcast.com\" http://localhost:5000/purchase_a_sword\n",
    "\tdocker-compose exec mids ab -n 15 -H \"Host: user2.comcast.com\" http://localhost:5000/join_guild\n",
    "\tdocker-compose exec mids ab -n 210 -H \"Host: user3.comcast.com\" http://localhost:5000/purchase_a_sword\n",
    "\tdocker-compose exec mids ab -n 7 -H \"Host: user3.comcast.com\" http://localhost:5000/join_guild\n",
    "\tsleep 10\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we submit our spark to event_stream. One is this done, you can watch while events stream as schedule. The code below ensure the events pass through the right schema, stored in `hdfs` and sink as described `sink to hdfs` above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose exec spark spark-submit /w205/project-3-GOhaike/full-stack/event_stream.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying events:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other to run query and perform analysis on streaming events, we are going to read from `hdfs` using a `hive metastore` in `presto` to register events in `hive` by running the command below. Our `event_stream.py` file contains two events of interest, `sword purchases and join guild` each of these events are filtered and stored in `hdfs`. Using `hive` these events are referenced and pass on to presto for querying and analysis.\n",
    "\n",
    "First, let's run a code to execute `hive` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose exec cloudera hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create external table `sword_purchases and join_guilds` by referencing `hdfs` to prepare data for `presto`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create external table if not exists default.sword_purchases (raw_event string,timestamp string,Accept string, Host string, User_Agent string, event_type string) stored as parquet location '/tmp/sword_purchase'  tblproperties (\"parquet.compress\"=\"SNAPPY\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create external table if not exists default.join_guilds (raw_event string,timestamp string,Accept string, Host string, User_Agent string, event_type string) stored as parquet location '/tmp/join_guild'  tblproperties (\"parquet.compress\"=\"SNAPPY\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then execute `presto` on server 8000 from hive catalog using a default schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose exec presto presto --server presto:8080 --catalog hive --schema default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's see what the table look like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show tables;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      Table      \n",
    "-----------------\n",
    " join_guilds     \n",
    " sword_purchases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows is a confirmation that we have twp tables `join_guilds and sword_purchases`. Now, we have successfully landed our streaming events to table where differents data analysis can be performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's see how the schema of sword purchases and join guilds look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe sword_purchases; \n",
    "    \n",
    "   Column   |  Type   | Comment \n",
    "------------+---------+---------\n",
    " raw_event  | varchar |         \n",
    " timestamp  | varchar |         \n",
    " accept     | varchar |         \n",
    " host       | varchar |         \n",
    " user_agent | varchar |         \n",
    " event_type | varchar |         \n",
    "(6 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe join_guilds;\n",
    "\n",
    "   Column   |  Type   | Comment \n",
    "------------+---------+---------\n",
    " raw_event  | varchar |         \n",
    " timestamp  | varchar |         \n",
    " accept     | varchar |         \n",
    " host       | varchar |         \n",
    " user_agent | varchar |         \n",
    " event_type | varchar |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both have the same structure. The column, type and comment. We can say they have the same schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate our stream events running `ab.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./ab.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we run the .sh, the streaming events will start to populate at any specified time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many sword purchases where made and how many join guilds?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select count(*) as sword_purchases FROM sword_purchases;\n",
    "      \n",
    " sword_purchases \n",
    "-----------------\n",
    "           91080 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many people that joined guild during the streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select count(*) as join_guild FROM join_guilds;\n",
    "    \n",
    "    \n",
    " join_guild \n",
    "------------\n",
    "       5344 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, `91080` purchased sword and `5344` joined guild. This is true as more request as made for sword purchases than join guild."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the first 10 rows of sword purchases and join guilds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presto:default> select host, event_type, timestamp from sword_purchases limit 10;\n",
    "     \n",
    "    \n",
    "       host        |   event_type   |        timestamp        \n",
    "-------------------+----------------+-------------------------\n",
    " user1.comcast.com | purchase_sword | 2020-08-01 01:17:18.513 \n",
    " user1.comcast.com | purchase_sword | 2020-08-01 01:17:18.52  \n",
    " user1.comcast.com | purchase_sword | 2020-08-01 01:17:18.526 \n",
    " user1.comcast.com | purchase_sword | 2020-08-01 01:17:18.532 \n",
    " user1.comcast.com | purchase_sword | 2020-08-01 01:17:18.539 \n",
    " user1.comcast.com | purchase_sword | 2020-08-01 01:17:18.545 \n",
    " user1.comcast.com | purchase_sword | 2020-08-01 01:17:18.55  \n",
    " user1.comcast.com | purchase_sword | 2020-08-01 01:17:18.555 \n",
    " user1.comcast.com | purchase_sword | 2020-08-01 01:17:18.561 \n",
    " user1.comcast.com | purchase_sword | 2020-08-01 01:17:18.565 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presto:default> select host, event_type, timestamp from join_guilds limit 10;\n",
    "    \n",
    "    \n",
    "       host        | event_type |        timestamp        \n",
    "-------------------+------------+-------------------------\n",
    " user1.comcast.com | join_guild | 2020-08-01 01:14:09.085 \n",
    " user1.comcast.com | join_guild | 2020-08-01 01:14:09.089 \n",
    " user1.comcast.com | join_guild | 2020-08-01 01:14:09.093 \n",
    " user1.comcast.com | join_guild | 2020-08-01 01:14:09.096 \n",
    " user1.comcast.com | join_guild | 2020-08-01 01:14:09.101 \n",
    " user1.comcast.com | join_guild | 2020-08-01 01:14:09.104 \n",
    " user1.comcast.com | join_guild | 2020-08-01 01:14:09.108 \n",
    " user1.comcast.com | join_guild | 2020-08-01 01:14:09.111 \n",
    " user1.comcast.com | join_guild | 2020-08-01 01:14:09.115 \n",
    " user1.comcast.com | join_guild | 2020-08-01 01:14:09.118 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many users do we have on our streaming events?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select count (DISTINCT host) as users FROM sword_purchases;\n",
    "    \n",
    " users \n",
    "-------\n",
    "     3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select count (DISTINCT host) as users FROM join_guilds;\n",
    "    \n",
    " users \n",
    "-------\n",
    "     3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both events, we have three users requests. More users are welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the frequency of host in sword purchases and join guilds events?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select host, count(*) as freq from sword_purchases GROUP BY host;\n",
    "\n",
    "    \n",
    "       host        | freq  \n",
    "-------------------+-------\n",
    " user1.comcast.com | 29700 \n",
    " user2.comcast.com | 19800 \n",
    " user3.comcast.com | 41580 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select host, count(*) as freq from join_guilds GROUP BY host;\n",
    "\n",
    "    \n",
    "       host        | freq \n",
    "-------------------+------\n",
    " user3.comcast.com | 1169 \n",
    " user1.comcast.com | 1670 \n",
    " user2.comcast.com | 2505 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both events, there are three users in the purchase swords and three users that join guilds. The `user3` made the most purchases while `user2` join most guilds. As expected, there are more purchases and join guilds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "We are able to successfully tracked these two events `purchasing a sword and joining guild` from the moment requests were made to what happend in the pipeline. We controlled the streaming from sword purchases by `15 mins` and join guild by `10`mins. Using Apache bench, we generated events and send to kafka. This events were stored in `hdfs` using `readstream` option from `kafka`. This allowed for continuosly streaming events and storing into `hdfs`. To examine what is in the `hdfs`, we used `hive metastore -the hard-way` to register events in a table and query using presto.\n",
    "\n",
    "Findings:\n",
    "\n",
    "_**There are two events :** sword purchases and join guilds_\n",
    "\n",
    "_There are `three users` in our stream events of which `91080` made sword purchases and `5344` joined guild_\n",
    "\n",
    "_In `purchase sword event`_\n",
    "\n",
    "   user1.comcast.com purchased `29700`\n",
    "   \n",
    "   user2.comcast.com purchased `19800`\n",
    "   \n",
    "   user3.comcast.com purchased `41580`\n",
    "\n",
    "\n",
    "_In `join guild event`_\n",
    "\n",
    "   user1.comcast.com joined `1169` times\n",
    "   \n",
    "   user2.comcast.com joined `1670` times\n",
    "   \n",
    "   user3.comcast.com joined `2505`times\n",
    "\n",
    "\n",
    "This concludes my mobile games tracking events. We are here to provide you with the latest game app with cutting edge technology.\n",
    "\n",
    "**Gabriel Ohaike**\n",
    "\n",
    "**Data Scientist**\n",
    "\n",
    "**Mobile game developer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
